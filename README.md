# TSCH-Q-Learning com Aprendizado Federado

Implementa√ß√£o de Q-Learning **com Aprendizado Federado** para otimiza√ß√£o de escalonamento em redes TSCH (Time Slotted Channel Hopping) usando Contiki-NG.

# Descri√ß√£o

Este projeto implementa um algoritmo de aprendizado por refor√ßo (Q-Learning) **combinado com Aprendizado Federado** para otimizar o escalonamento de slots de tempo em redes TSCH. Os n√≥s da rede **compartilham e agregam** seus conhecimentos (Q-tables) para acelerar o aprendizado coletivo e melhorar a efici√™ncia global da rede.

O sistema aprende dinamicamente a melhor aloca√ß√£o de slots baseando-se em m√©tricas de desempenho como:

- Taxas de transmiss√£o e recep√ß√£o bem-sucedidas
- Gerenciamento de buffer
- Detec√ß√£o e penaliza√ß√£o de conflitos
- Throughput da rede
- **Conhecimento agregado de m√∫ltiplos n√≥s (Federated Learning)**

# Estrutura do Projeto

```
TSCH-Q-Learning/
‚îú‚îÄ‚îÄ examples/           # C√≥digo de exemplo e configura√ß√µes
‚îÇ   ‚îú‚îÄ‚îÄ node.c         # Implementa√ß√£o do n√≥ sensor com FL
‚îÇ   ‚îú‚îÄ‚îÄ Makefile       # Build system
‚îÇ   ‚îî‚îÄ‚îÄ project-conf.h # Configura√ß√µes do projeto
‚îú‚îÄ‚îÄ net/               # Estruturas de rede
‚îÇ   ‚îú‚îÄ‚îÄ queuebuf.c    # Gerenciamento de buffer de pacotes
‚îÇ   ‚îî‚îÄ‚îÄ queuebuf.h
‚îú‚îÄ‚îÄ tsch/              # M√≥dulos TSCH, Q-Learning e Federated Learning
‚îÇ   ‚îú‚îÄ‚îÄ q-learning.c          # Implementa√ß√£o do algoritmo Q-Learning
‚îÇ   ‚îú‚îÄ‚îÄ q-learning.h
‚îÇ   ‚îú‚îÄ‚îÄ federated-learning.c  # Implementa√ß√£o do Aprendizado Federado
‚îÇ   ‚îú‚îÄ‚îÄ federated-learning.h
‚îÇ   ‚îú‚îÄ‚îÄ customized-tsch-file.c # Extens√µes TSCH customizadas
‚îÇ   ‚îú‚îÄ‚îÄ customized-tsch-file.h
‚îÇ   ‚îú‚îÄ‚îÄ tsch-slot-operation.c  # Opera√ß√µes de slots TSCH
‚îÇ   ‚îú‚îÄ‚îÄ tsch-slot-operation.h
‚îÇ   ‚îî‚îÄ‚îÄ tsch.h
‚îî‚îÄ‚îÄ logs/              # Logs de execu√ß√£o
    ‚îî‚îÄ‚îÄ loglistener_qlearning-05-12.txt
```

# Caracter√≠sticas

## Aprendizado Federado (Federated Learning)
- **Comunica√ß√£o P2P**: N√≥s compartilham Q-tables via UDP (porta 8766)
- **M√©todos de Agrega√ß√£o**:
  - **FedAvg**: M√©dia simples das Q-tables
  - **Weighted FedAvg**: M√©dia ponderada baseada em experi√™ncia (padr√£o)
  - **FedMedian**: Mediana robusta a outliers
- **Sincroniza√ß√£o**: A cada 180 segundos (configur√°vel)
- **M√°ximo de Vizinhos**: 10 n√≥s (configur√°vel)
- **Limpeza Autom√°tica**: Remove vizinhos inativos ap√≥s timeout
- **Balanceamento Local/Global**: Peso configur√°vel entre modelo local e federado

## Q-Learning
- Tabela Q com tamanho configur√°vel (padr√£o: tamanho do slotframe)
- Taxa de aprendizado (learning rate): 0.1
- Fator de desconto (discount factor): 0.9
- Fun√ß√£o de recompensa baseada em:
  - Œ∏‚ÇÅ = 3.0 (peso para transmiss√µes bem-sucedidas)
  - Œ∏‚ÇÇ = 1.5 (peso para gerenciamento de buffer)
  - Œ∏‚ÇÉ = 0.5 (peso para conflitos)
  - Penalidade de conflito = 100.0

## TSCH
- Escalonamento din√¢mico de slots
- Detec√ß√£o de conflitos
- Suporte para m√∫ltiplos tipos de dados (UNICAST, BROADCAST, EB)
- Fila de status de pacotes customizada

# Configura√ß√£o

## Pr√©-requisitos

- Contiki-NG
- Compilador GCC para arquitetura alvo
- Make

## Par√¢metros Configur√°veis

### Q-Learning
```c
// Tamanho da tabela Q-value
#define Q_VALUE_LIST_SIZE TSCH_SCHEDULE_DEFAULT_LENGTH

// Tamanho da fila de transmiss√£o
#define MAX_NUMBER_OF_CUSTOM_QUEUE 20

// Habilitar impress√£o de registros
#define PRINT_TRANSMISSION_RECORDS 1
```

### Aprendizado Federado
```c
// Habilitar/desabilitar aprendizado federado
#define ENABLE_FEDERATED_LEARNING 1

// N√∫mero m√°ximo de vizinhos
#define MAX_FEDERATED_NEIGHBORS 10

// Intervalo de sincroniza√ß√£o (segundos)
#define FEDERATED_SYNC_INTERVAL 180

// M√©todos dispon√≠veis: FEDAVG, WEIGHTED_FEDAVG, FEDMEDIAN
federated_learning_init(WEIGHTED_FEDAVG);

// Peso do modelo local (0.0 a 1.0)
set_local_model_weight(0.5);  // 50% local, 50% federado
```

# Compila√ß√£o e Execu√ß√£o

## Compilar o Projeto

```bash
cd examples/
make TARGET=<seu-target>
```

### Upload para o Hardware

```bash
make TARGET=<seu-target> node.upload
```

## Executar no Cooja Simulator

1. Abra o Cooja
2. Crie uma nova simula√ß√£o
3. Adicione n√≥s com o firmware compilado
4. Configure a rede TSCH
5. Inicie a simula√ß√£o

# Fun√ß√£o de Recompensa

A fun√ß√£o de recompensa TSCH √© calculada como:

```
R = Œ∏‚ÇÅ √ó (n_tx + n_rx) - Œ∏‚ÇÇ √ó buffer_penalty - conflict_penalty √ó n_conflicts
```

Onde:
- `n_tx`: n√∫mero de transmiss√µes bem-sucedidas
- `n_rx`: n√∫mero de recep√ß√µes bem-sucedidas
- `buffer_penalty`: penalidade baseada no tamanho do buffer
- `n_conflicts`: n√∫mero de conflitos detectados

# Monitoramento

Os logs s√£o gerados em tempo de execu√ß√£o e incluem:
- Registros de transmiss√£o/recep√ß√£o com n√∫meros de slot
- Atualiza√ß√£o da tabela Q
- Detec√ß√£o de conflitos
- Status do buffer
- **üì° Broadcast e recep√ß√£o de Q-tables**
- **üîó Agrega√ß√£o federada com estat√≠sticas**
- **üë• N√∫mero de vizinhos ativos**
- **‚öñÔ∏è M√©todo de agrega√ß√£o utilizado**

Os logs s√£o salvos na pasta `logs/`.

### Exemplo de Log Federado
```
[INFO: FedLearn] Federated Learning initialized with method=1
[INFO: FedLearn] Received Q-table from node 3 (samples=15)
[INFO: FedLearn] Broadcasting Q-table (samples=12)
[INFO: FedLearn] Weighted FedAvg: local_weight=0.44, neighbors=2
[INFO: FedLearn] Federated aggregation complete: neighbors=2, method=1, local_samples=12
```

# Processos

## UDP Communication Process
- Porta UDP: 8765 (dados de aplica√ß√£o)
- Intervalo de envio: 60 segundos
- Gerencia comunica√ß√£o entre n√≥s

## RL-TSCH Scheduler Process
- Intervalo de atualiza√ß√£o da tabela Q: 120 segundos
- Setup do escalonamento m√≠nimo: 120 segundos
- Implementa o algoritmo de aprendizado

## Federated Learning Sync Process ‚≠ê NOVO
- Porta UDP: 8766 (compartilhamento de Q-tables)
- Intervalo de sincroniza√ß√£o: 180 segundos
- Broadcast de Q-tables locais
- Agrega√ß√£o de conhecimento de vizinhos
- Limpeza de entradas obsoletas

# Estruturas de Dados

## `env_state`
Armazena o estado do ambiente:
- `buffer_size`: tamanho do buffer atual
- `energy_level`: n√≠vel de energia do n√≥

## `packet_status`
Rastreia status de transmiss√£o de pacotes:
- `data_type`: tipo de dados (UNICAST, BROADCAST, EB)
- `packet_seqno`: n√∫mero de sequ√™ncia
- `transmission_count`: contagem de transmiss√µes
- `time_slot`: slot de tempo usado
- `channel_offset`: offset do canal
- `node_id`: ID do n√≥
- `trans_addr`: endere√ßo de transmiss√£o

# API Principal

## Aprendizado Federado
```c
// Inicializar sistema federado
void federated_learning_init(fed_aggregation_method_t method);

// Armazenar Q-table de vizinho
uint8_t store_neighbor_q_table(uint16_t node_id, float *q_values, uint8_t num_samples);

// Agregar Q-tables (chama m√©todo configurado)
uint8_t federated_aggregate(void);

// Agrega√ß√£o FedAvg (m√©dia simples)
uint8_t federated_aggregate_fedavg(void);

// Agrega√ß√£o ponderada por experi√™ncia
uint8_t federated_aggregate_weighted(void);

// Agrega√ß√£o por mediana (robusta)
uint8_t federated_aggregate_median(void);

// Obter Q-table local para compartilhar
float* get_local_q_table_for_sharing(void);

// Incrementar contador de amostras locais
void increment_local_samples(void);

// Limpar vizinhos obsoletos
void cleanup_stale_neighbors(uint32_t timeout_seconds);

// Configurar m√©todo de agrega√ß√£o
void set_aggregation_method(fed_aggregation_method_t method);

// Configurar peso do modelo local (0.0 - 1.0)
void set_local_model_weight(float weight);

// Obter estat√≠sticas federadas
void get_federated_stats(uint8_t *num_neighbors, uint8_t *local_samples, 
                         fed_aggregation_method_t *method);
```

## Q-Learning
```c
// Retorna a a√ß√£o com maior Q-value
uint8_t get_highest_q_val(void);

// Atualiza a tabela Q
void update_q_table(uint8_t action, float got_reward);

// Calcula recompensa TSCH
float tsch_reward_function(uint8_t n_tx, uint8_t n_rx, 
                          uint8_t n_buff_prev, 
                          uint8_t n_buff_new, 
                          uint8_t n_conflicts);
```

## Gerenciamento de Fila
```c
// Adiciona pacote √† fila
void enqueue(queue_packet_status *queue, packet_status pkt_sts);

// Verifica se fila est√° vazia/cheia
int isEmpty(queue_packet_status *queue);
int isFull(queue_packet_status *queue);
```

# Desempenho

O algoritmo Q-Learning **com Aprendizado Federado** aprende continuamente para:
- ‚úÖ Maximizar throughput
- ‚úÖ Minimizar conflitos de slot
- ‚úÖ Otimizar uso do buffer
- ‚úÖ Melhorar efici√™ncia energ√©tica
- ‚úÖ **Acelerar converg√™ncia atrav√©s de conhecimento compartilhado**
- ‚úÖ **Melhorar robustez com agrega√ß√£o de m√∫ltiplos n√≥s**
- ‚úÖ **Adaptar-se mais r√°pido a mudan√ßas na topologia da rede**

## Benef√≠cios do Aprendizado Federado

### üöÄ Converg√™ncia Mais R√°pida
Cada n√≥ aprende n√£o apenas com suas pr√≥prias experi√™ncias, mas tamb√©m com as experi√™ncias de seus vizinhos, acelerando significativamente o processo de aprendizado.

### üõ°Ô∏è Maior Robustez
A agrega√ß√£o de m√∫ltiplas Q-tables reduz o impacto de experi√™ncias at√≠picas ou ru√≠do em n√≥s individuais.

### üîÑ Adapta√ß√£o Din√¢mica
A rede se adapta melhor a mudan√ßas topol√≥gicas, pois o conhecimento √© distribu√≠do e continuamente atualizado.

### üìä Privacidade Preservada
Apenas as Q-tables s√£o compartilhadas, n√£o os dados brutos dos pacotes ou informa√ß√µes sens√≠veis.

### ‚öñÔ∏è Balanceamento Configur√°vel
O par√¢metro `aggregation_weight` permite ajustar o equil√≠brio entre conhecimento local e federado.

# Contribuindo

Contribui√ß√µes s√£o bem-vindas! Sinta-se √† vontade para:
- Reportar bugs
- Sugerir novas funcionalidades
- Melhorar documenta√ß√£o
- Submeter pull requests

# Licen√ßa

Este projeto √© fornecido para fins educacionais e de pesquisa.

# Autores

Desenvolvido como parte de pesquisa em otimiza√ß√£o de redes TSCH usando aprendizado por refor√ßo.

# Refer√™ncias

- Contiki-NG: https://github.com/contiki-ng/contiki-ng
- TSCH: IEEE 802.15.4e Time Slotted Channel Hopping
- Q-Learning: Sutton & Barto - Reinforcement Learning
- **Federated Learning: McMahan et al. - Communication-Efficient Learning of Deep Networks from Decentralized Data (2017)**
- **FedAvg: Federated Averaging Algorithm**

## Artigos Relacionados
- **"Federated Reinforcement Learning for IoT Networks"**
- **"Distributed Q-Learning in Wireless Sensor Networks"**
- **"Privacy-Preserving Machine Learning in WSNs"**

---

**Nota**: Este √© um projeto de pesquisa em desenvolvimento com suporte a **Aprendizado Federado**. Para uso em produ√ß√£o, testes adicionais e otimiza√ß√µes s√£o recomendados.

## üÜï Novidades da Vers√£o Federada

### v2.0 - Aprendizado Federado
- ‚úÖ Implementa√ß√£o completa de Federated Learning
- ‚úÖ Tr√™s m√©todos de agrega√ß√£o (FedAvg, Weighted, Median)
- ‚úÖ Comunica√ß√£o UDP para compartilhamento de Q-tables
- ‚úÖ Sistema de limpeza de vizinhos obsoletos
- ‚úÖ Estat√≠sticas detalhadas de agrega√ß√£o
- ‚úÖ Configura√ß√£o flex√≠vel de par√¢metros
- ‚úÖ Preserva√ß√£o de privacidade (apenas Q-tables compartilhadas)